{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning of VGG16 for Biofuel Material Cassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras as kr\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = './dataset/'\n",
    "TRAIN_FOLDER = 'train'\n",
    "VALIDATION_FOLDER = 'validation'\n",
    "TEST_FOLDER = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image processing & Input preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDG = kr.preprocessing.image.ImageDataGenerator\n",
    "TR_IDG = IDG(rescale=1)\n",
    "VA_IDG = IDG(rescale=1)\n",
    "TE_IDG = IDG(rescale=1)\n",
    "\n",
    "# Generator for training data set\n",
    "train_set = TR_IDG.flow_from_directory(\n",
    "    DATASET_ROOT + TRAIN_FOLDER, \n",
    "    target_size=(224,224), \n",
    "    batch_size=10\n",
    ");\n",
    "\n",
    "# Generator for validation data set\n",
    "validation_set = VA_IDG.flow_from_directory(\n",
    "    DATASET_ROOT + VALIDATION_FOLDER, \n",
    "    target_size=(224,224), \n",
    "    batch_size=10\n",
    ");\n",
    "\n",
    "test_batch_size = 20\n",
    "\n",
    "# Generator for test data set\n",
    "test_set = TE_IDG.flow_from_directory(\n",
    "    DATASET_ROOT + TEST_FOLDER, \n",
    "    target_size=(224,224), \n",
    "    batch_size=test_batch_size\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to display data set and its labels\n",
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "\tif type(ims[0]) is np.ndarray:\n",
    "\t\tims = np.array(ims).astype(np.uint8)\n",
    "\t\tif (ims.shape[-1] != 3):\n",
    "\t\t\tims = ims.transpose((0,2,3,1))\n",
    "\tf = plt.figure(figsize=figsize)\n",
    "\tcols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "\tfor i in range(len(ims)):\n",
    "\t\tsp = f.add_subplot(rows, cols, i+1)\n",
    "\t\tsp.axis('Off')\n",
    "\t\tif titles is not None:\n",
    "\t\t\tsp.set_title(titles[i], fontsize=16)\n",
    "\t\tplt.imshow(ims[i], interpolation=None if interp else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next() gets batches of size batch_size\n",
    "imgs, labels = next(train_set)\n",
    "# display the batch and its labels\n",
    "plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch & Download VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = kr.applications.vgg16\n",
    "vgg16_model = vgg16.VGG16(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data from a pretrained model (skip if training)\n",
    "_If running the cell below, skip the build & train phases!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = kr.models.load_model(\"trained_model\")\n",
    "history = []\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building & Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the layers\n",
    "for layer in vgg16_model.layers[:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create the model\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg16_model)\n",
    "\n",
    "# Ease of Access\n",
    "Dropout, Flatten, Dense = kr.layers.Dropout, kr.layers.Flatten, kr.layers.Dense\n",
    "\n",
    "# Add new layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile Module\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=kr.optimizers.RMSprop(lr=1e-4),\n",
    "    metrics=['acc']\n",
    ")\n",
    "\n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warmup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the head of the module (Our Layers)\n",
    "t1_history = model.fit_generator(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.samples/train_set.batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=validation_set.samples/validation_set.batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset validaiton & train set generators\n",
    "train_set.reset()\n",
    "validation_set.reset()\n",
    "\n",
    "# Now that the head FC layers have been trained/initialized, lets\n",
    "# Unfreeze the final set of CONV layers and make them trainable\n",
    "for layer in vgg16_model.layers[15:]:\n",
    "\tlayer.trainable = True\n",
    "    \n",
    "# Recompile module for changes to take effect, now using SGD with very small learning rate\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=kr.optimizers.SGD(lr=1e-4, momentum=0.9), \n",
    "    metrics=[\"accuracy\"]\n",
    ")    \n",
    "\n",
    "# Train the whole module\n",
    "t2_history = model.fit_generator(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.samples/train_set.batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=validation_set.samples/validation_set.batch_size,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_history = [t1_history, t2_history]\n",
    "\n",
    "for history in global_history: \n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions & Visualising Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_set, steps=1, verbose=1)\n",
    "_, test_labels = next(test_set)\n",
    "\n",
    "# get predictions for the test set\n",
    "for i in range(len(test_set)-1):\n",
    "    predictions = np.concatenate((predictions,model.predict_generator(test_set, steps=1, verbose=1)))\n",
    "    _,temp = next(test_set)\n",
    "    test_labels = np.concatenate((test_labels,temp))\n",
    "\n",
    "# get labels from the test set\n",
    "def ReformatData(y):\n",
    "    x = np.copy(y[:,0])\n",
    "    for i in range(len(x)):\n",
    "        if x[i]>=0.5 or x[i] == True:\n",
    "            x[i]=1\n",
    "        else:\n",
    "            x[i]=0\n",
    "    x.astype(int)        \n",
    "    return x\n",
    "                \n",
    "cut_predictions = ReformatData(predictions)\n",
    "cut_labels = ReformatData(test_labels)\n",
    "\n",
    "print(\"Cut predictions:\",cut_predictions)\n",
    "print(\"Cut labels:\",cut_labels)\n",
    "print(\"Unique labels:\",unique_labels(cut_predictions,cut_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = [\"non_biofuel\",\"biofuel\"]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, abs(i-0.25), format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix based on predictions of our test data set\n",
    "plot_confusion_matrix(cut_labels, cut_predictions, ['biomass', 'non_biomass'], title='Confusion Matrix', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------------- Cells Below Are for Testing Purposes ----------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### DONT RUN - THIS IS FOR TESTING PURPOSES ########\n",
    "\n",
    "# load the model\n",
    "test_vgg16 = kr.applications.vgg16\n",
    "test_model = test_vgg16.VGG16()\n",
    "# load an image from file\n",
    "test_image = kr.preprocessing.image.load_img('dataset/train/biomass/cardboard189.jpg', target_size=(224, 224))\n",
    "# convert the image pixels to a numpy array\n",
    "test_image = kr.preprocessing.image.img_to_array(test_image)\n",
    "# reshape data for the model\n",
    "test_image = test_image.reshape((1, test_image.shape[0], test_image.shape[1], test_image.shape[2]))\n",
    "# prepare the image for the VGG model\n",
    "test_image = test_vgg16.preprocess_input(test_image)\n",
    "# predict the probability across all output classes\n",
    "yhat = test_model.predict(test_image)\n",
    "# convert the probabilities to class labels\n",
    "tesT_label = test_vgg16.decode_predictions(yhat)\n",
    "# retrieve the most likely result, e.g. highest probability\n",
    "test_label = test_label[0][0]\n",
    "# print the classification\n",
    "print('%s (%.2f%%)' % (test_label[1], test_label[2]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
