{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation & Augementation\n",
    "In this file the dataset is loaded and transformed into all the formats required by the different classification algorithms. When calling this file, **make sure** you have a few global constants set:\n",
    "* REQUIRED_DIMENSIONS\n",
    "* TRAIN_BATCH_SIZE\n",
    "* VALIDATION_BATCH_SIZE \n",
    "* TEST_BATCH_SIZE\n",
    "* N_AUG_VS_TEST\n",
    "\n",
    "In addition, this file creates output constants:\n",
    "* DATASET_ROOT\n",
    "* TRAIN_FOLDER      \n",
    "* VALIDATION_FOLDER \n",
    "* TEST_FOLDER\n",
    "* CLASSES\n",
    "\n",
    "And, output generators:\n",
    "* test_set\n",
    "* train_set\n",
    "* valdiation_set\n",
    "* (augmentation versions)\n",
    "\n",
    "And, output raw congregates:\n",
    "* rawImages\n",
    "* rawFeatures\n",
    "* rawLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Correct File Usage Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "import tensorflow as tf\n",
    "import matplotlib as mp\n",
    "\n",
    "from matplotlib import pyplot as pp\n",
    "from tensorflow import keras as kr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this file by itself, this variables must be defined\n",
    "# in the file importing this as they are implementation dependant\n",
    "try:\n",
    "    (REQUIRED_DIMENSIONS \n",
    "         or TRAIN_BATCH_SIZE \n",
    "         or VALIDATION_BATCH_SIZE \n",
    "         or TEST_BATCH_SIZE\n",
    "         or N_AUG_VS_TEST)\n",
    "except NameError:\n",
    "    raise Exception('One of the required global constants is missing.\\n -> Do not run this file by itself?')\n",
    "\n",
    "# Implementation independent globals\n",
    "DATASET_ROOT      = './dataset/'\n",
    "TRAIN_FOLDER      = 'train'\n",
    "VALIDATION_FOLDER = 'validation'\n",
    "TEST_FOLDER       = 'test'\n",
    "CLASSES           = ['biomass', 'non_biomass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "lib.log(\"Data preparation started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n",
    "# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n",
    "sometimes = lambda aug: ia.augmenters.Sometimes(0.5, aug)\n",
    "\n",
    "# Augment training set\n",
    "seq = ia.augmenters.Sequential([\n",
    "    # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    ia.augmenters.Crop(px=(0, 16)),\n",
    "    # horizontally flip 50% of the images\n",
    "    ia.augmenters.Fliplr(0.5),\n",
    "    # vertically flip 20% of all image\n",
    "    ia.augmenters.Flipud(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(img):\n",
    "    seq_det = seq.to_deterministic()\n",
    "    aug_image = seq_det.augment_image(img)\n",
    "\n",
    "    return aug_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool to display data set and its labels\n",
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "\tif type(ims[0]) is np.ndarray:\n",
    "\t\tims = np.array(ims).astype(np.uint8)\n",
    "\t\tif (ims.shape[-1] != 3):\n",
    "\t\t\tims = ims.transpose((0,2,3,1))\n",
    "\tf = pp.figure(figsize=figsize)\n",
    "\tcols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "\tfor i in range(len(ims)):\n",
    "\t\tsp = f.add_subplot(rows, cols, i+1)\n",
    "\t\tsp.axis('Off')\n",
    "\t\tif titles is not None:\n",
    "\t\t\tsp.set_title(titles[i], fontsize=16)\n",
    "\t\tpp.imshow(ims[i], interpolation=None if interp else 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=REQUIRED_DIMENSIONS):\n",
    "\t# resize the image to a fixed size, then flatten the image into\n",
    "\t# a list of raw pixel intensities\n",
    "\treturn cv2.resize(image, size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_histogram(image, bins=(8, 8, 8)):\n",
    "\t# extract a 3D color histogram from the HSV color space using\n",
    "\t# the supplied number of `bins` per channel\n",
    "\thsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\thist = cv2.calcHist([hsv], [0, 1, 2], None, bins,\n",
    "\t\t[0, 180, 0, 256, 0, 256])\n",
    " \n",
    "\t# handle normalizing the histogram if we are using OpenCV 2.4.X\n",
    "\tif imutils.is_cv2():\n",
    "\t\thist = cv2.normalize(hist)\n",
    " \n",
    "\t# otherwise, perform \"in place\" normalization in OpenCV 3 (I\n",
    "\t# personally hate the way this is done\n",
    "\telse:\n",
    "\t\tcv2.normalize(hist, hist)\n",
    " \n",
    "\t# return the flattened histogram as the feature vector\n",
    "\treturn hist.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.log(\"Loaded custom functions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Existing Dataset & Perform Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Raw Congregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Congregate\n",
    "rawImages = []\n",
    "rawFeatures = []\n",
    "rawLabels = []\n",
    "\n",
    "for setName in os.listdir(DATASET_ROOT):\n",
    "    setPath = DATASET_ROOT + setName\n",
    "    for className in os.listdir(setPath):\n",
    "        classPath = setPath + \"/\" + className\n",
    "        for imageName in os.listdir(classPath):\n",
    "            imagePath = classPath + \"/\" + imageName\n",
    "            # load the image and extract the class label (assuming that our\n",
    "            # path as the format: /path/to/dataset/{class}.{image_num}.jpg\n",
    "            image = cv2.imread(imagePath)\n",
    "            label = className\n",
    "            \n",
    "            if not hasattr(image, \"__len__\"):\n",
    "                print(\"(SKIPPED) Found a non image file: \", imagePath)\n",
    "                continue\n",
    "            \n",
    "            # extract raw pixel intensity \"features\", followed by a color\n",
    "            # histogram to characterize the color distribution of the pixels\n",
    "            # in the image\n",
    "            pixels = image_to_feature_vector(image)\n",
    "            hist = extract_color_histogram(image)\n",
    "\n",
    "            # update the raw images, features, and labels matricies,\n",
    "            # respectively\n",
    "            rawImages.append(pixels)\n",
    "            rawFeatures.append(hist)\n",
    "            rawLabels.append(label)\n",
    "\n",
    "# Make numpy arrays, because more useful\n",
    "rawImages = np.array(rawImages)\n",
    "rawFeatures = np.array(rawFeatures)\n",
    "rawLabels = np.array(rawLabels)\n",
    "\n",
    "# Display some useful information\n",
    "print(\"Number of images in total: \" + str(len(rawImages)))\n",
    "print(\"Raw pixels matrix: {:.2f}MB\".format(rawImages.nbytes / (1024 * 1000.0)))\n",
    "print(\"Raw features matrix: {:.2f}MB\".format(rawFeatures.nbytes / (1024 * 1000.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.log(\"Loaded raw congregates of images, features and labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Keras Generators & Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shortcut\n",
    "IDG = kr.preprocessing.image.ImageDataGenerator\n",
    "\n",
    "# No augmentation\n",
    "generator = IDG()\n",
    "\n",
    "# Augmentation\n",
    "agumented_generator = IDG(preprocessing_function=augment)\n",
    "\n",
    "# Iterator for training data set\n",
    "train_set = generator.flow_from_directory(\n",
    "    DATASET_ROOT + TRAIN_FOLDER, \n",
    "    target_size=REQUIRED_DIMENSIONS, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Iterator for validation data set\n",
    "validation_set = generator.flow_from_directory(\n",
    "    DATASET_ROOT + VALIDATION_FOLDER, \n",
    "    target_size=REQUIRED_DIMENSIONS, \n",
    "    batch_size=VALIDATION_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Iterator for test data set\n",
    "test_set = generator.flow_from_directory(\n",
    "    DATASET_ROOT + TEST_FOLDER, \n",
    "    target_size=REQUIRED_DIMENSIONS, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ");\n",
    "\n",
    "# Iterator for augmented training data set\n",
    "augmented_train_set = agumented_generator.flow_from_directory(\n",
    "    DATASET_ROOT + TRAIN_FOLDER, \n",
    "    target_size=REQUIRED_DIMENSIONS, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.log(\"Loaded all generators.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising: Augmented Data VS Non Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.log(\"Displaying first \" + str(N_AUG_VS_TEST) + \" batches of the augmented_train_set VS train_set:\")\n",
    "\n",
    "for i in range(N_AUG_VS_TEST):\n",
    "    imgs, labels = next(augmented_train_set)\n",
    "    plots(imgs, titles=labels)\n",
    "\n",
    "for i in range(N_AUG_VS_TEST):\n",
    "    imgs, labels = next(train_set)\n",
    "    plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib.log(\"Data preperation completed!\\nTime taken: \" + str(time.time() - start_time) + \" seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
